{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tC4jbs5yqzYw"
   },
   "source": [
    "# Task1\n",
    "\n",
    "This tutorial walks through the process of running the CAD1 Task1 baseline using the shell Interface\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffET3AfZFKPt"
   },
   "source": [
    "The python and shell scripts included in the repository make use of <a href='https://hydra.cc/'>Hydra</a> and <a href='https://hydra.cc/docs/plugins/submitit_launcher/'>Submitit</a>, two technologies which streamline the configuration and parallel operation of python code on both local and high performance computing (HPC) environments.\n",
    "\n",
    "The use of hydra for configuration allows for the existing shell scripts to be easily redirected to include new audio data and modify the various parameters of the recipe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pajpylpbFud6"
   },
   "source": [
    "## 1. Cloning the Clarity Repository\n",
    "We first need to install the Clarity package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "is_executing": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!git clone --quiet https://github.com/claritychallenge/clarity.git\n",
    "%cd clarity\n",
    "!git checkout v0.3.2\n",
    "%pip install -e .\n",
    "%cd recipes/cad1/task1/baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8pXJVSt-F-NN"
   },
   "source": [
    "## 2. Getting the demo data\n",
    "\n",
    "We will be using music audio and listener metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10323,
     "status": "ok",
     "timestamp": 1677842984853,
     "user": {
      "displayName": "Cadenza Challenge",
      "userId": "12951360330666177503"
     },
     "user_tz": 0
    },
    "id": "TpG-bGs8Fzgl",
    "outputId": "438666a1-ac21-4532-a340-baefee85202d"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import gdown\n",
    "\n",
    "!gdown 10SfuZR7yVlVO6RwNUc3kPeJHGiwpN3VS\n",
    "!mv cadenza_data_demo.tar.xz recipes/cad1/task1/baseline\n",
    "!tar -xvf cadenza_data_demo.tar.xz\n",
    "!rm cadenza_data_demo.tar.xz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline recipe is divided into 2 stage; **Enhnacement** and **Evaluation**\n",
    "\n",
    "```{image} images/headphone_simple_v2.png\n",
    ":alt: Task1\n",
    ":width: 800px\n",
    ":align: center\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.1 Enhancement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The enhancement script contains several key functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Enhance \n",
    "The main function that orchestrate the demixing and remixing.\n",
    "\n",
    "- It takes the config.yaml as input parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "@hydra.main(config_path=\"\", config_name=\"config\")\n",
    "def enhance(config: DictConfig) -> None:\n",
    "    \"\"\"\n",
    "    Run the music enhancement.\n",
    "    The system decomposes the music into vocal, drums, bass, and other stems.\n",
    "    Then, the NAL-R prescription procedure is applied to each stem.\n",
    "    Args:\n",
    "        config (dict): Dictionary of configuration options for enhancing music.\n",
    "\n",
    "    Returns 8 stems for each song:\n",
    "        - left channel vocal, drums, bass, and other stems\n",
    "        - right channel vocal, drums, bass, and other stems\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Decompose Signal \n",
    "It takes a music signal and a pretrained model decompose the signal into eight stems (VDBO).\n",
    "\n",
    "- The baseline takes either the demucs or the Open-UnMix model acording to the value of the `separator.model` parameter in the config.yaml\n",
    "- The function has the left and right audiograms as input parameters. This is to show that one could train a model using this information to produces a personalizes decomposition.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def decompose_signal(\n",
    "    model: torch.nn.Module,\n",
    "    model_sample_rate: int,\n",
    "    signal: np.ndarray,\n",
    "    signal_sample_rate: int,\n",
    "    device: torch.device,\n",
    "    sources_list: list[str],\n",
    "    left_audiogram: np.ndarray,\n",
    "    right_audiogram: np.ndarray,\n",
    "    normalise: bool = True,\n",
    ") -> dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Decompose signal into 8 stems.\n",
    "\n",
    "    The left and right audiograms are ignored by the baseline system as it\n",
    "    is performing personalised decomposition.\n",
    "    Instead, it performs a standard music decomposition using the\n",
    "    HDEMUCS model trained on the MUSDB18 dataset.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Torch model.\n",
    "        model_sample_rate (int): Sample rate of the model.\n",
    "        signal (np.ndarray): Signal to be decomposed.\n",
    "        signal_sample_rate (int): Sample frequency.\n",
    "        device (torch.device): Torch device to use for processing.\n",
    "        sources_list (list): List of strings used to index dictionary.\n",
    "        left_audiogram (np.ndarray): Left ear audiogram.\n",
    "        right_audiogram (np.ndarray): Right ear audiogram.\n",
    "        normalise (bool): Whether to normalise the signal.\n",
    "\n",
    "     Returns:\n",
    "         Dictionary: Indexed by sources with the associated model as values.\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Process Stems for Listeners \n",
    "Function that takes the stems from the demixing and process them to the target listener.\n",
    "\n",
    "- In the baseline it applies NAL-R prescription to each stem\n",
    "- The use of this function may becomes unnecessary in the case of having a separation model that performs a personalized decomposition. E.g., the model applies the NAL-R amplification as part of the decomposition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def process_stems_for_listener(\n",
    "    stems: dict,\n",
    "    enhancer: NALR,\n",
    "    compressor: Compressor,\n",
    "    audiogram_left: np.ndarray,\n",
    "    audiogram_right: np.ndarray,\n",
    "    cfs: np.ndarray,\n",
    "    apply_compressor: bool = False,\n",
    ") -> dict:\n",
    "    \"\"\"Process the stems from sources.\n",
    "\n",
    "    Args:\n",
    "        stems (dict) : Dictionary of stems\n",
    "        enhancer (NALR) : NAL-R prescription hearing aid\n",
    "        compressor (Compressor) : Compressor\n",
    "        audiogram_left (np.ndarray) : Left channel audiogram\n",
    "        audiogram_right (np.ndarray) : Right channel audiogram\n",
    "        cfs (np.ndarray) : Center frequencies\n",
    "        apply_compressor (bool) : Whether to apply the compressor\n",
    "\n",
    "    Returns:\n",
    "        processed_sources (dict) : Dictionary of processed stems\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.4 Remix Signal\n",
    "\n",
    "Function that generates the remix \n",
    "\n",
    "- The baseline just add all processed signals \n",
    "- This function can be modify to apply any approach you may want to explore.\n",
    "    - Change the level of the stems\n",
    "    - Rebalance left and right channels.\n",
    "    - ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def remix_signal(stems: dict) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function to remix signal. It takes the eight stems\n",
    "    and combines them into a stereo signal.\n",
    "\n",
    "    Args:\n",
    "        stems (dict) : Dictionary of stems\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray) : Remixed signal\n",
    "\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.5 Utility Functions\n",
    "\n",
    "These are function that only help to separate general operation. \n",
    "\n",
    "* separate_sources : Performs separation inference using any separation model. \n",
    "* map_to_dict : Reorganised the separated stems into a dictionary\n",
    "* get_device : Returns the Torch Device ('cuda' or 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation script assumes that exists the `exp` output directory resulting from runing the `enhancement.py` script. \n",
    "\n",
    "The evaluation script `cannot` be modified or altered in any way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Run calculate aq\n",
    "\n",
    "Main function that orchestrate the evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "@hydra.main(config_path=\"\", config_name=\"config\")\n",
    "def run_calculate_aq(config: DictConfig) -> None:\n",
    "    \"\"\"Evaluate the enhanced signals using the HAAQI metric.\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Set Song Seed\n",
    "\n",
    "Function to seed the random camponents of the HAAQI score.\n",
    "This ensures a fair comparison between different systems as the seed depends on the song and not on the order of when that song was processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def set_song_seed(song: str) -> None:\n",
    "    \"\"\"Set a seed that is unique for the given song\"\"\"\n",
    "    song_encoded = hashlib.md5(song.encode(\"utf-8\")).hexdigest()\n",
    "    song_md5 = int(song_encoded, 16) % (10**8)\n",
    "    np.random.seed(song_md5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Evaluate Song Listener\n",
    "\n",
    "Computes the evaluation of a single song-listener pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def _evaluate_song_listener(\n",
    "    song: str,\n",
    "    listener: str,\n",
    "    config: DictConfig,\n",
    "    split_dir: str,\n",
    "    listener_audiograms: dict,\n",
    "    enhanced_folder: Path,\n",
    ") -> tuple[float, dict]:\n",
    "    \"\"\"Evaluate a single song-listener pair\n",
    "\n",
    "    Args:\n",
    "        song (str): The name of the song to evaluate.\n",
    "        listener (str): The name of the listener to evaluate.\n",
    "        config (DictConfig): The configuration object.\n",
    "        split_dir (str): The name of the split directory.\n",
    "        listener_audiograms (dict): A dictionary of audiograms for each listener.\n",
    "        enhanced_folder (Path): The path to the folder containing the enhanced signals.\n",
    "\n",
    "    Returns:\n",
    "        combined_score (float): The combined score for the result.\n",
    "        per_instrument_score (dict): A dictionary of scores for each\n",
    "            instrument channel in the result.\n",
    "\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWFEskB3HgGM"
   },
   "source": [
    "## 4. Inspecting Existing Configuration\n",
    "\n",
    "All of the included shell scripts take configurable variables from the yaml files in the same directory as the shell script.Typically these are named `config.yaml`, however, other names may be used if more than one shell script is in a directory.\n",
    "\n",
    "We can inspect the contents of the config file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 485,
     "status": "ok",
     "timestamp": 1677843014925,
     "user": {
      "displayName": "Cadenza Challenge",
      "userId": "12951360330666177503"
     },
     "user_tz": 0
    },
    "id": "CwQbLMMXGsdp",
    "is_executing": true,
    "outputId": "fe5611e3-f7f2-4402-eabe-e3e06429b187"
   },
   "source": [
    "```yaml\n",
    "path:\n",
    "  root: ../../cadenza_data_demo/cad1/task1\n",
    "  metadata_dir: ${path.root}/metadata\n",
    "  music_dir: ${path.root}/audio/musdb18hq\n",
    "  music_train_file: ${path.metadata_dir}/musdb18.train.json\n",
    "  music_valid_file: ${path.metadata_dir}/musdb18.valid.json\n",
    "  listeners_train_file: ${path.metadata_dir}/listeners.train.json\n",
    "  listeners_valid_file: ${path.metadata_dir}/listeners.valid.json\n",
    "  exp_folder: ./exp_${separator.model} # folder to store enhanced signals and final results\n",
    "\n",
    "sample_rate: 44100       # sample rate of the input mixture\n",
    "stem_sample_rate: 24000  # sample rate output stems\n",
    "remix_sample_rate: 32000 # sample rate for output remixed signal\n",
    "\n",
    "nalr:\n",
    "  nfir: 220\n",
    "  fs: ${sample_rate}\n",
    "\n",
    "apply_compressor: False\n",
    "compressor:\n",
    "  threshold: 0.35\n",
    "  attenuation: 0.1\n",
    "  attack: 50\n",
    "  release: 1000\n",
    "  rms_buffer_size: 0.064\n",
    "\n",
    "soft_clip: True\n",
    "\n",
    "separator:\n",
    "  model: demucs   # demucs or openunmix\n",
    "  device: ~\n",
    "\n",
    "evaluate:\n",
    "  set_random_seed: True\n",
    "  small_test: False\n",
    "  batch_size: 1  # Number of batches\n",
    "  batch: 0       # Batch number to evaluate\n",
    "\n",
    "# hydra config\n",
    "hydra:\n",
    "  run:\n",
    "    dir: ${path.exp_folder}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PorM62C9HpQq"
   },
   "source": [
    "The general organisation of the config files is hierarchical, with property labels depending on the script in question. The config file for the enhance and evaluate recipes contains configurable paramaters for both scripts. These include:\n",
    "- Paths for the locations of audio files, metadata and the export location for generated files\n",
    "- Paramaters for the NAL-R fitting\n",
    "- Paramaters for the automatic gain control (AGC) compressor used in the baseline enhancer\n",
    "- Parameters for the challenge evaluator\n",
    "- Parameters necessary for Hydra to run\n",
    "\n",
    "The path.root parameter defaults to the root of the baseline and must be overrided with a dataset root path when the python script is called in the command line.\n",
    "\n",
    "e.g\n",
    "\n",
    "```\n",
    "user:~$ python mypythonscript.py path.root='/path/to/project' \n",
    "```\n",
    "\n",
    "In this notebook we will use the environment variable <code>$NBOOKROOT</code> which we defined at the start of the tutorial.\n",
    "\n",
    "Note the lack of slash at the end of the <code>path.root</code> argument string. If you inspect a variable such as <code>path.metadata_dir</code> you will see that this slash is already included in the line.\n",
    "\n",
    "```\n",
    "path:\n",
    "  root: ./\n",
    "  metadata_dir: ${path.root}/task1/metadata\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7nvI7seIlQH"
   },
   "source": [
    "The general form for overriding a parameter in the CLI is dot indexed. For the following entry in a <code>config.yaml</code> file:\n",
    "```yaml\n",
    "A:\n",
    "  B:\n",
    "    parameter_0: some_value\n",
    "    parameter_1: some_other_value\n",
    "```\n",
    "The CLI syntax to override those values would be:\n",
    "\n",
    "```bash\n",
    "User:~$ python myscript.py A.B.parameter_0=\"new_value\" A.B.parameter_1=\"another_new_value\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCGGqlrjIo9w"
   },
   "source": [
    "## 5. Run Demo \n",
    "\n",
    "Typically, as stated above, all the work is done within python with configurable variables supplied by a <code>yaml</code> file which is parsed by Hydra inside the python code. \n",
    "\n",
    "The execution of this code is performed in the CLI and new configuration variable values are supplied as arguments to override defaults. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quq_sa9fIyTK"
   },
   "source": [
    "We are now ready to run the prepared python script. However, the standard configuration is designed to work with the full clarity dataset. We can redirect the script to the correct folders by overriding the appropriate configuration parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NnDcNpa-Iv03",
    "is_executing": true,
    "outputId": "aabb3c37-4f25-4d32-dc1b-e2fb9644e366"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-17 17:28:54,811][torchaudio.utils.download][INFO] - The local file (/home/gerardo/.cache/torch/hub/torchaudio/models/hdemucs_high_musdbhq_only.pt) exists. Skipping the download.\n",
      "[2023-05-17 17:28:56,710][__main__][INFO] - [001/002] Processing Actions - One Minute Smile for L5076...\n",
      "[2023-05-17 17:29:04,906][__main__][INFO] - [002/002] Processing Actions - One Minute Smile for L5040...\n"
     ]
    }
   ],
   "source": [
    "!python enhance.py path.root=../cadenza_data_demo/cad1/task1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The enhancement results in eight stems and one remix for all listener-song pairs.\n",
    "\n",
    "```{image} images/task1_output.png\n",
    ":alt: Task1\n",
    ":width: 800px\n",
    ":align: center\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMKBDkc4Nsl_"
   },
   "source": [
    "Now that we have enhanced audios we can use the evaluate recipe to generate HAAQI scores for the signals. The evaluation is run in the same manner as the enhancement script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFvYiF15LJEu",
    "is_executing": true
   },
   "source": [
    "```bash\n",
    "!python evaluate.py path.root=../cadenza_data_demo/cad1/task1\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO6zJdm1xg5M576cwlW17h0",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
